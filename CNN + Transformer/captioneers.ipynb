{"cells":[{"cell_type":"code","source":[""],"metadata":{"id":"oaRpPZikrdr4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g6_PlbDVrgL5","executionInfo":{"status":"ok","timestamp":1653682225332,"user_tz":-180,"elapsed":22817,"user":{"displayName":"Svetlana Pavlova","userId":"05514837240559826181"}},"outputId":"0f529a2a-de9b-4b33-b733-74337ee87ecf"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#%cd /content/drive/MyDrive/a-PyTorch-Tutorial-to-Image-Captioning\n","%cd /content/drive/MyDrive/PyTorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FvHm4sXWsDRA","executionInfo":{"status":"ok","timestamp":1653682230340,"user_tz":-180,"elapsed":382,"user":{"displayName":"Svetlana Pavlova","userId":"05514837240559826181"}},"outputId":"336502b9-d6e5-47c6-d180-270ac1cd6b04"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/PyTorch\n"]}]},{"cell_type":"code","source":["!ls\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZxS9Pg2sKEm","executionInfo":{"status":"ok","timestamp":1653682236377,"user_tz":-180,"elapsed":501,"user":{"displayName":"Svetlana Pavlova","userId":"05514837240559826181"}},"outputId":"f64c9a19-ea29-4df1-844b-b15f077f8798"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["BEST_checkpoint_flickr30k_5_cap_per_img_5_min_word_freq.pth.tar\n","captioneers.ipynb\n","caption.py\n","checkpoint_flickr30k_5_cap_per_img_5_min_word_freq.pth.tar\n","create_input_files.py\n","datasets.py\n","enlarge_resnet.json\n","eval.py\n","flickr_prep\n","gan_model.py\n","img\n","models.py\n","__pycache__\n","TRAIN_IMAGES_flickr30k_5_cap_per_img_5_min_word_freq-003.hdf5\n","train.py\n","train_utils.py\n","utils.py\n"]}]},{"cell_type":"code","execution_count":26,"metadata":{"id":"RHKVb6lErYss","executionInfo":{"status":"ok","timestamp":1653659908088,"user_tz":-180,"elapsed":292,"user":{"displayName":"Svetlana Pavlova","userId":"05514837240559826181"}}},"outputs":[],"source":["#import argparse\n","#parser = argparse.ArgumentParser(description='Process some integers.')\n","#parser.add_argument('--device', type=str)\n","#args = parser.parse_args()\n","\n","from datasets import CaptionDataset\n","from models import *\n","from utils import *\n","from train_utils import *\n","import torch.optim\n","import torch.utils.data\n","import torchvision.transforms as transforms\n","import torch.backends.cudnn as cudnn\n","import os\n","import json\n","import numpy as np"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SXX7eKZSrYsx","executionInfo":{"status":"ok","timestamp":1653659910096,"user_tz":-180,"elapsed":280,"user":{"displayName":"Svetlana Pavlova","userId":"05514837240559826181"}},"outputId":"7243115f-7d80-496b-cbe5-ab104811966b"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["#os.environ['CUDA_VISIBLE_DEVICES'] = args.device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/a-PyTorch-Tutorial-to-Image-Captioning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5-ZcEIauO11","executionInfo":{"status":"ok","timestamp":1653659911123,"user_tz":-180,"elapsed":3,"user":{"displayName":"Svetlana Pavlova","userId":"05514837240559826181"}},"outputId":"82e8a4f0-670a-46f0-fd79-9cc67730a016"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1qbeagtpdhUjJ0qiqM0snKrjUhBUYcQFB/a-PyTorch-Tutorial-to-Image-Captioning\n"]}]},{"cell_type":"code","execution_count":29,"metadata":{"id":"aiSQFFaCrYsy","executionInfo":{"status":"ok","timestamp":1653659918346,"user_tz":-180,"elapsed":6198,"user":{"displayName":"Svetlana Pavlova","userId":"05514837240559826181"}}},"outputs":[],"source":["# Data parameters\n","#main_folder = \"./a-PyTorch-Tutorial-to-Image-Captioning\"\n","\n","# data_folder = '/media/ssd/caption data'  # folder with data files saved by create_input_files.py\n","data_folder = f'flickr_prep/'  # folder with data files saved by create_input_files.py\n","# data_name = 'coco_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n","data_name = 'flickr30k_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n","word_map_file = f'flickr_prep/WORDMAP_flickr30k_5_cap_per_img_5_min_word_freq.json'\n","with open(word_map_file, 'r') as j:\n","    word_map = json.load(j)\n","\n","# Model parameters\n","emb_dim = 512  # dimension of word embeddings\n","attention_dim = 512  # dimension of attention linear layers\n","decoder_dim = 512  # dimension of decoder RNN\n","dropout = 0.5\n","n_heads = 8\n","encoder_layers = 2\n","decoder_layers = 6\n","attention_method = \"ByPixel\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n","cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n","\n","# Training parameters\n","start_epoch = 0\n","epochs = 10  # number of epochs to train for (if early stopping is not triggered)\n","epochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\n","batch_size = 64\n","workers = 0  # for data-loading; right now, only 1 works with h5py\n","encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n","decoder_lr = 4e-4  # learning rate for decoder\n","grad_clip = 5.  # clip gradients at an absolute value of\n","alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n","best_bleu4 = 0.  # BLEU-4 score right now\n","print_freq = 100  # print training/validation stats every __ batches\n","fine_tune_encoder = False  # fine-tune encoder?\n","checkpoint = None  # path to checkpoint, None if none\n","\n","decoder = Transformer(vocab_size=len(word_map),\n","                        embed_dim=emb_dim,\n","                        encoder_layers=encoder_layers,\n","                        decoder_layers=decoder_layers,\n","                        dropout=dropout,\n","                        attention_method=attention_method,\n","                        n_heads=n_heads)\n","\n","decoder = decoder.to(device)\n","\n","decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n","                                     lr=decoder_lr)\n","encoder = CNN_Encoder()\n","encoder.fine_tune(fine_tune_encoder)\n","encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n","                                     lr=encoder_lr) if fine_tune_encoder else None\n","encoder = encoder.to(device)\n","\n","# Loss function\n","criterion = nn.CrossEntropyLoss(ignore_index = 0).to(device)\n","\n","# Custom dataloaders\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","train_loader = torch.utils.data.DataLoader(\n","    CaptionDataset(data_folder, data_name, 'TRAIN', transform=transforms.Compose([normalize])),\n","    batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n","val_loader = torch.utils.data.DataLoader(\n","    CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n","    batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n","\n","save_name = 'enlarge_resnet.json'\n","training_track = {'loss':[],'bleu':[]}"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/PyTorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FlUSevVmuX_3","executionInfo":{"status":"ok","timestamp":1653659919197,"user_tz":-180,"elapsed":4,"user":{"displayName":"Svetlana Pavlova","userId":"05514837240559826181"}},"outputId":"6e9e33d6-147e-4090-9d0c-ba0a8ab0fc60"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/PyTorch\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GvgdbhL2rYs0","outputId":"89f33d0e-0185-4411-95f4-a84ab784b2d0"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch: [0][0/2266]\tBatch Time 2.517 (2.517)\tData Load Time 0.526 (0.526)\tLoss 9.5918 (9.5918)\tTop-5 Accuracy 0.113 (0.113)\n","Epoch: [0][100/2266]\tBatch Time 2.315 (2.308)\tData Load Time 0.260 (0.264)\tLoss 6.0170 (6.3699)\tTop-5 Accuracy 30.626 (28.692)\n","Epoch: [0][200/2266]\tBatch Time 2.318 (2.302)\tData Load Time 0.268 (0.256)\tLoss 6.1959 (6.2697)\tTop-5 Accuracy 28.390 (28.806)\n","Epoch: [0][300/2266]\tBatch Time 2.284 (2.299)\tData Load Time 0.240 (0.252)\tLoss 6.1832 (6.2257)\tTop-5 Accuracy 28.656 (28.986)\n","Epoch: [0][400/2266]\tBatch Time 2.274 (2.295)\tData Load Time 0.227 (0.248)\tLoss 5.9820 (6.1662)\tTop-5 Accuracy 31.096 (29.740)\n","Epoch: [0][500/2266]\tBatch Time 2.291 (2.290)\tData Load Time 0.263 (0.246)\tLoss 5.9607 (6.1167)\tTop-5 Accuracy 33.531 (30.323)\n","Epoch: [0][600/2266]\tBatch Time 2.268 (2.288)\tData Load Time 0.246 (0.244)\tLoss 5.8630 (6.0791)\tTop-5 Accuracy 34.479 (30.771)\n","Epoch: [0][700/2266]\tBatch Time 2.289 (2.285)\tData Load Time 0.249 (0.242)\tLoss 5.7272 (6.0468)\tTop-5 Accuracy 36.052 (31.163)\n","Epoch: [0][800/2266]\tBatch Time 2.267 (2.284)\tData Load Time 0.230 (0.242)\tLoss 5.9130 (6.0187)\tTop-5 Accuracy 32.167 (31.483)\n","Epoch: [0][900/2266]\tBatch Time 2.304 (2.283)\tData Load Time 0.273 (0.241)\tLoss 5.8777 (5.9942)\tTop-5 Accuracy 32.755 (31.753)\n","Epoch: [0][1000/2266]\tBatch Time 2.279 (2.282)\tData Load Time 0.234 (0.240)\tLoss 5.4870 (5.9704)\tTop-5 Accuracy 36.534 (31.985)\n","Epoch: [0][1100/2266]\tBatch Time 2.289 (2.281)\tData Load Time 0.243 (0.239)\tLoss 5.8095 (5.9505)\tTop-5 Accuracy 33.181 (32.184)\n","Epoch: [0][1200/2266]\tBatch Time 2.246 (2.280)\tData Load Time 0.202 (0.239)\tLoss 5.6815 (5.9345)\tTop-5 Accuracy 34.038 (32.334)\n","Epoch: [0][1300/2266]\tBatch Time 2.261 (2.279)\tData Load Time 0.234 (0.238)\tLoss 5.6149 (5.9191)\tTop-5 Accuracy 35.804 (32.472)\n","Epoch: [0][1400/2266]\tBatch Time 2.293 (2.278)\tData Load Time 0.251 (0.238)\tLoss 5.7074 (5.9065)\tTop-5 Accuracy 34.686 (32.579)\n","Epoch: [0][1500/2266]\tBatch Time 2.287 (2.277)\tData Load Time 0.251 (0.237)\tLoss 5.5792 (5.8954)\tTop-5 Accuracy 36.183 (32.679)\n","Epoch: [0][1600/2266]\tBatch Time 2.299 (2.277)\tData Load Time 0.259 (0.237)\tLoss 5.7531 (5.8838)\tTop-5 Accuracy 32.037 (32.784)\n","Epoch: [0][1700/2266]\tBatch Time 2.251 (2.276)\tData Load Time 0.219 (0.236)\tLoss 5.7344 (5.8733)\tTop-5 Accuracy 32.839 (32.875)\n","Epoch: [0][1800/2266]\tBatch Time 2.253 (2.275)\tData Load Time 0.230 (0.236)\tLoss 5.5714 (5.8640)\tTop-5 Accuracy 36.242 (32.956)\n","Epoch: [0][1900/2266]\tBatch Time 2.270 (2.275)\tData Load Time 0.230 (0.236)\tLoss 5.7368 (5.8546)\tTop-5 Accuracy 32.997 (33.027)\n","Epoch: [0][2000/2266]\tBatch Time 2.272 (2.275)\tData Load Time 0.225 (0.236)\tLoss 5.5662 (5.8463)\tTop-5 Accuracy 33.924 (33.102)\n","Epoch: [0][2100/2266]\tBatch Time 2.252 (2.275)\tData Load Time 0.214 (0.235)\tLoss 5.5782 (5.8374)\tTop-5 Accuracy 37.344 (33.182)\n","Epoch: [0][2200/2266]\tBatch Time 2.271 (2.275)\tData Load Time 0.235 (0.235)\tLoss 5.6365 (5.8298)\tTop-5 Accuracy 35.731 (33.255)\n","Validation: [0/80]\tBatch Time 3.646 (3.646)\tLoss 10.6815 (10.6815)\tTop-5 Accuracy 17.928 (17.928)\t\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"]},{"output_type":"stream","name":"stdout","text":["\n"," * LOSS - 10.600, TOP-5 ACCURACY - 18.344, BLEU-4 - 0.6770720407899168\n","\n","Epoch: [1][0/2266]\tBatch Time 3.046 (3.046)\tData Load Time 0.991 (0.991)\tLoss 5.5104 (5.5104)\tTop-5 Accuracy 34.957 (34.957)\n","Epoch: [1][100/2266]\tBatch Time 2.272 (2.347)\tData Load Time 0.239 (0.309)\tLoss 5.8351 (5.6116)\tTop-5 Accuracy 33.054 (35.332)\n","Epoch: [1][200/2266]\tBatch Time 2.312 (2.317)\tData Load Time 0.271 (0.279)\tLoss 5.3784 (5.6106)\tTop-5 Accuracy 38.040 (35.359)\n","Epoch: [1][300/2266]\tBatch Time 2.262 (2.307)\tData Load Time 0.231 (0.270)\tLoss 5.5269 (5.6131)\tTop-5 Accuracy 35.723 (35.312)\n","Epoch: [1][400/2266]\tBatch Time 2.299 (2.298)\tData Load Time 0.266 (0.262)\tLoss 5.4686 (5.6130)\tTop-5 Accuracy 38.421 (35.325)\n","Epoch: [1][500/2266]\tBatch Time 2.266 (2.293)\tData Load Time 0.240 (0.257)\tLoss 5.5204 (5.6087)\tTop-5 Accuracy 36.121 (35.402)\n","Epoch: [1][600/2266]\tBatch Time 2.209 (2.288)\tData Load Time 0.214 (0.255)\tLoss 5.5914 (5.6074)\tTop-5 Accuracy 35.853 (35.395)\n","Epoch: [1][700/2266]\tBatch Time 2.204 (2.278)\tData Load Time 0.229 (0.253)\tLoss 5.7113 (5.6034)\tTop-5 Accuracy 36.180 (35.428)\n","Epoch: [1][800/2266]\tBatch Time 2.208 (2.271)\tData Load Time 0.222 (0.251)\tLoss 5.7291 (5.6037)\tTop-5 Accuracy 33.813 (35.467)\n","Epoch: [1][900/2266]\tBatch Time 2.207 (2.266)\tData Load Time 0.233 (0.250)\tLoss 5.6281 (5.6023)\tTop-5 Accuracy 32.925 (35.483)\n","Epoch: [1][1000/2266]\tBatch Time 2.213 (2.261)\tData Load Time 0.205 (0.249)\tLoss 5.8134 (5.6004)\tTop-5 Accuracy 32.459 (35.500)\n","Epoch: [1][1100/2266]\tBatch Time 2.189 (2.258)\tData Load Time 0.215 (0.248)\tLoss 5.4331 (5.5954)\tTop-5 Accuracy 37.429 (35.553)\n","Epoch: [1][1200/2266]\tBatch Time 2.207 (2.255)\tData Load Time 0.234 (0.247)\tLoss 5.4296 (5.5915)\tTop-5 Accuracy 37.965 (35.583)\n","Epoch: [1][1300/2266]\tBatch Time 2.218 (2.252)\tData Load Time 0.238 (0.246)\tLoss 5.4981 (5.5875)\tTop-5 Accuracy 36.804 (35.623)\n","Epoch: [1][1400/2266]\tBatch Time 2.199 (2.249)\tData Load Time 0.221 (0.245)\tLoss 5.6489 (5.5861)\tTop-5 Accuracy 35.978 (35.639)\n","Epoch: [1][1500/2266]\tBatch Time 2.237 (2.247)\tData Load Time 0.257 (0.245)\tLoss 5.5382 (5.5836)\tTop-5 Accuracy 35.379 (35.655)\n","Epoch: [1][1600/2266]\tBatch Time 2.234 (2.244)\tData Load Time 0.253 (0.244)\tLoss 5.5114 (5.5806)\tTop-5 Accuracy 36.482 (35.673)\n","Epoch: [1][1700/2266]\tBatch Time 2.180 (2.241)\tData Load Time 0.226 (0.243)\tLoss 5.4984 (5.5774)\tTop-5 Accuracy 36.085 (35.714)\n","Epoch: [1][1800/2266]\tBatch Time 2.167 (2.239)\tData Load Time 0.212 (0.243)\tLoss 5.4007 (5.5744)\tTop-5 Accuracy 36.684 (35.738)\n","Epoch: [1][1900/2266]\tBatch Time 2.191 (2.236)\tData Load Time 0.238 (0.242)\tLoss 5.4092 (5.5722)\tTop-5 Accuracy 36.842 (35.761)\n","Epoch: [1][2000/2266]\tBatch Time 2.183 (2.234)\tData Load Time 0.224 (0.242)\tLoss 5.4077 (5.5706)\tTop-5 Accuracy 36.977 (35.775)\n","Epoch: [1][2100/2266]\tBatch Time 2.176 (2.232)\tData Load Time 0.223 (0.241)\tLoss 5.4534 (5.5679)\tTop-5 Accuracy 36.806 (35.797)\n","Epoch: [1][2200/2266]\tBatch Time 2.211 (2.230)\tData Load Time 0.240 (0.241)\tLoss 5.6022 (5.5650)\tTop-5 Accuracy 34.419 (35.820)\n","Validation: [0/80]\tBatch Time 1.251 (1.251)\tLoss 10.7388 (10.7388)\tTop-5 Accuracy 19.053 (19.053)\t\n","\n"," * LOSS - 10.734, TOP-5 ACCURACY - 17.802, BLEU-4 - 0.6770720407899168\n","\n","\n","Epochs since last improvement: 1\n","\n","Epoch: [2][0/2266]\tBatch Time 2.629 (2.629)\tData Load Time 0.642 (0.642)\tLoss 5.3762 (5.3762)\tTop-5 Accuracy 36.070 (36.070)\n","Epoch: [2][100/2266]\tBatch Time 2.183 (2.245)\tData Load Time 0.223 (0.291)\tLoss 5.4280 (5.4964)\tTop-5 Accuracy 38.320 (35.964)\n","Epoch: [2][200/2266]\tBatch Time 2.184 (2.219)\tData Load Time 0.219 (0.263)\tLoss 5.6040 (5.5020)\tTop-5 Accuracy 34.101 (36.131)\n","Epoch: [2][300/2266]\tBatch Time 2.191 (2.213)\tData Load Time 0.228 (0.254)\tLoss 5.4838 (5.5006)\tTop-5 Accuracy 36.692 (36.182)\n","Epoch: [2][400/2266]\tBatch Time 2.209 (2.209)\tData Load Time 0.240 (0.249)\tLoss 5.4716 (5.4954)\tTop-5 Accuracy 35.958 (36.202)\n","Epoch: [2][500/2266]\tBatch Time 2.192 (2.206)\tData Load Time 0.208 (0.245)\tLoss 5.4626 (5.4957)\tTop-5 Accuracy 37.455 (36.226)\n","Epoch: [2][600/2266]\tBatch Time 2.170 (2.206)\tData Load Time 0.206 (0.243)\tLoss 5.5375 (5.4952)\tTop-5 Accuracy 36.085 (36.240)\n","Epoch: [2][700/2266]\tBatch Time 2.206 (2.205)\tData Load Time 0.235 (0.241)\tLoss 5.5382 (5.4944)\tTop-5 Accuracy 36.703 (36.230)\n","Epoch: [2][800/2266]\tBatch Time 2.185 (2.204)\tData Load Time 0.224 (0.240)\tLoss 5.4854 (5.4937)\tTop-5 Accuracy 35.987 (36.231)\n","Epoch: [2][900/2266]\tBatch Time 2.175 (2.202)\tData Load Time 0.205 (0.239)\tLoss 5.2682 (5.4934)\tTop-5 Accuracy 39.674 (36.221)\n","Epoch: [2][1000/2266]\tBatch Time 2.228 (2.202)\tData Load Time 0.266 (0.239)\tLoss 5.5221 (5.4932)\tTop-5 Accuracy 35.738 (36.215)\n","Epoch: [2][1100/2266]\tBatch Time 2.226 (2.202)\tData Load Time 0.248 (0.238)\tLoss 5.5833 (5.4924)\tTop-5 Accuracy 35.401 (36.217)\n","Epoch: [2][1200/2266]\tBatch Time 2.200 (2.203)\tData Load Time 0.228 (0.238)\tLoss 5.4848 (5.4911)\tTop-5 Accuracy 35.682 (36.236)\n","Epoch: [2][1300/2266]\tBatch Time 2.229 (2.203)\tData Load Time 0.262 (0.238)\tLoss 5.5758 (5.4915)\tTop-5 Accuracy 35.782 (36.244)\n","Epoch: [2][1400/2266]\tBatch Time 2.208 (2.203)\tData Load Time 0.228 (0.237)\tLoss 5.4263 (5.4911)\tTop-5 Accuracy 36.829 (36.256)\n","Epoch: [2][1500/2266]\tBatch Time 2.211 (2.203)\tData Load Time 0.250 (0.237)\tLoss 5.4704 (5.4910)\tTop-5 Accuracy 35.691 (36.259)\n","Epoch: [2][1600/2266]\tBatch Time 2.203 (2.203)\tData Load Time 0.226 (0.237)\tLoss 5.6603 (5.4897)\tTop-5 Accuracy 35.381 (36.270)\n","Epoch: [2][1700/2266]\tBatch Time 2.206 (2.203)\tData Load Time 0.236 (0.237)\tLoss 5.4540 (5.4882)\tTop-5 Accuracy 38.038 (36.289)\n","Epoch: [2][1800/2266]\tBatch Time 2.211 (2.204)\tData Load Time 0.245 (0.237)\tLoss 5.4361 (5.4872)\tTop-5 Accuracy 34.881 (36.305)\n","Epoch: [2][1900/2266]\tBatch Time 2.209 (2.204)\tData Load Time 0.228 (0.237)\tLoss 5.4194 (5.4863)\tTop-5 Accuracy 36.982 (36.309)\n","Epoch: [2][2000/2266]\tBatch Time 2.202 (2.204)\tData Load Time 0.232 (0.237)\tLoss 5.6390 (5.4859)\tTop-5 Accuracy 34.132 (36.311)\n","Epoch: [2][2100/2266]\tBatch Time 2.218 (2.204)\tData Load Time 0.261 (0.237)\tLoss 5.6881 (5.4851)\tTop-5 Accuracy 36.278 (36.324)\n","Epoch: [2][2200/2266]\tBatch Time 2.186 (2.203)\tData Load Time 0.216 (0.236)\tLoss 5.5529 (5.4841)\tTop-5 Accuracy 33.333 (36.340)\n","Validation: [0/80]\tBatch Time 1.282 (1.282)\tLoss 10.5838 (10.5838)\tTop-5 Accuracy 20.605 (20.605)\t\n","\n"," * LOSS - 10.912, TOP-5 ACCURACY - 19.024, BLEU-4 - 0.6770720407899168\n","\n","\n","Epochs since last improvement: 2\n","\n","Epoch: [3][0/2266]\tBatch Time 2.456 (2.456)\tData Load Time 0.462 (0.462)\tLoss 5.3213 (5.3213)\tTop-5 Accuracy 37.107 (37.107)\n","Epoch: [3][100/2266]\tBatch Time 2.213 (2.255)\tData Load Time 0.243 (0.284)\tLoss 5.4077 (5.4326)\tTop-5 Accuracy 37.948 (36.620)\n","Epoch: [3][200/2266]\tBatch Time 2.183 (2.229)\tData Load Time 0.219 (0.259)\tLoss 5.5131 (5.4371)\tTop-5 Accuracy 35.731 (36.435)\n","Epoch: [3][300/2266]\tBatch Time 2.193 (2.219)\tData Load Time 0.223 (0.249)\tLoss 5.5932 (5.4457)\tTop-5 Accuracy 33.073 (36.395)\n","Epoch: [3][400/2266]\tBatch Time 2.268 (2.215)\tData Load Time 0.227 (0.245)\tLoss 5.5627 (5.4436)\tTop-5 Accuracy 34.088 (36.453)\n","Epoch: [3][500/2266]\tBatch Time 2.271 (2.223)\tData Load Time 0.246 (0.241)\tLoss 5.6003 (5.4405)\tTop-5 Accuracy 34.603 (36.533)\n","Epoch: [3][600/2266]\tBatch Time 2.258 (2.231)\tData Load Time 0.241 (0.243)\tLoss 5.4665 (5.4392)\tTop-5 Accuracy 37.799 (36.532)\n","Epoch: [3][700/2266]\tBatch Time 2.261 (2.236)\tData Load Time 0.237 (0.242)\tLoss 5.2873 (5.4398)\tTop-5 Accuracy 38.269 (36.529)\n","Epoch: [3][800/2266]\tBatch Time 2.239 (2.240)\tData Load Time 0.212 (0.242)\tLoss 5.3578 (5.4425)\tTop-5 Accuracy 38.768 (36.523)\n","Epoch: [3][900/2266]\tBatch Time 2.469 (2.243)\tData Load Time 0.440 (0.241)\tLoss 5.5378 (5.4428)\tTop-5 Accuracy 34.149 (36.531)\n","Epoch: [3][1000/2266]\tBatch Time 2.236 (2.245)\tData Load Time 0.201 (0.241)\tLoss 5.4728 (5.4449)\tTop-5 Accuracy 35.646 (36.535)\n","Epoch: [3][1100/2266]\tBatch Time 2.263 (2.247)\tData Load Time 0.237 (0.241)\tLoss 5.5164 (5.4442)\tTop-5 Accuracy 36.461 (36.539)\n","Epoch: [3][1200/2266]\tBatch Time 2.250 (2.249)\tData Load Time 0.225 (0.241)\tLoss 5.2073 (5.4426)\tTop-5 Accuracy 37.330 (36.556)\n","Epoch: [3][1300/2266]\tBatch Time 2.266 (2.250)\tData Load Time 0.228 (0.241)\tLoss 5.5465 (5.4425)\tTop-5 Accuracy 34.479 (36.560)\n","Epoch: [3][1400/2266]\tBatch Time 2.236 (2.250)\tData Load Time 0.216 (0.240)\tLoss 5.3492 (5.4409)\tTop-5 Accuracy 37.425 (36.591)\n","Epoch: [3][1500/2266]\tBatch Time 2.237 (2.250)\tData Load Time 0.215 (0.239)\tLoss 5.6471 (5.4416)\tTop-5 Accuracy 34.500 (36.573)\n","Epoch: [3][1600/2266]\tBatch Time 2.254 (2.251)\tData Load Time 0.232 (0.238)\tLoss 5.5852 (5.4419)\tTop-5 Accuracy 34.978 (36.561)\n","Epoch: [3][1700/2266]\tBatch Time 2.245 (2.251)\tData Load Time 0.211 (0.237)\tLoss 5.4336 (5.4420)\tTop-5 Accuracy 35.202 (36.556)\n","Epoch: [3][1800/2266]\tBatch Time 2.236 (2.251)\tData Load Time 0.210 (0.236)\tLoss 5.3892 (5.4400)\tTop-5 Accuracy 36.737 (36.574)\n","Epoch: [3][1900/2266]\tBatch Time 2.246 (2.251)\tData Load Time 0.222 (0.236)\tLoss 5.4160 (5.4400)\tTop-5 Accuracy 37.037 (36.576)\n","Epoch: [3][2000/2266]\tBatch Time 2.254 (2.251)\tData Load Time 0.228 (0.235)\tLoss 5.3589 (5.4402)\tTop-5 Accuracy 36.215 (36.575)\n","Epoch: [3][2100/2266]\tBatch Time 2.254 (2.251)\tData Load Time 0.222 (0.235)\tLoss 5.3918 (5.4394)\tTop-5 Accuracy 37.440 (36.599)\n","Epoch: [3][2200/2266]\tBatch Time 2.241 (2.251)\tData Load Time 0.214 (0.234)\tLoss 5.4859 (5.4392)\tTop-5 Accuracy 35.976 (36.602)\n","Validation: [0/80]\tBatch Time 1.272 (1.272)\tLoss 10.8293 (10.8293)\tTop-5 Accuracy 20.047 (20.047)\t\n","\n"," * LOSS - 11.129, TOP-5 ACCURACY - 18.811, BLEU-4 - 0.6770720407899168\n","\n","\n","Epochs since last improvement: 3\n","\n","Epoch: [4][0/2266]\tBatch Time 2.419 (2.419)\tData Load Time 0.392 (0.392)\tLoss 5.5125 (5.5125)\tTop-5 Accuracy 36.344 (36.344)\n","Epoch: [4][100/2266]\tBatch Time 2.268 (2.309)\tData Load Time 0.238 (0.282)\tLoss 5.4732 (5.4187)\tTop-5 Accuracy 36.949 (36.724)\n","Epoch: [4][200/2266]\tBatch Time 2.249 (2.284)\tData Load Time 0.227 (0.257)\tLoss 5.5903 (5.4196)\tTop-5 Accuracy 34.955 (36.535)\n"]}],"source":["# Epochs\n","for epoch in range(start_epoch, epochs):\n","\n","    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n","    if epochs_since_improvement == 20:\n","        break\n","    if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n","        adjust_learning_rate(decoder_optimizer, 0.8)\n","        if fine_tune_encoder:\n","            adjust_learning_rate(encoder_optimizer, 0.8)\n","\n","    # One epoch's training\n","    train(train_loader=train_loader,\n","          encoder=encoder,\n","          decoder=decoder,\n","          criterion=criterion,\n","          encoder_optimizer=encoder_optimizer,\n","          decoder_optimizer=decoder_optimizer,\n","          epoch=epoch,alpha_c=alpha_c,\n","          print_freq=print_freq,\n","          grad_clip=grad_clip,\n","          n_heads=n_heads,\n","          decoder_layers=decoder_layers)\n","\n","    # One epoch's validation\n","    recent_bleu4, loss = validate(val_loader=val_loader,\n","                            encoder=encoder,\n","                            decoder=decoder,\n","                            criterion=criterion,\n","                            alpha_c=alpha_c,\n","                            print_freq=print_freq,\n","                            word_map=word_map,\n","                            n_heads=n_heads,\n","                            decoder_layers=decoder_layers)\n","\n","    training_track['bleu'].append(recent_bleu4)\n","    training_track['loss'].append(loss)\n","    with open(save_name, 'w') as f:\n","        json.dump(training_track, f)\n","\n","    # Check if there was an improvement\n","    is_best = recent_bleu4 > best_bleu4\n","    best_bleu4 = max(recent_bleu4, best_bleu4)\n","    if not is_best:\n","        epochs_since_improvement += 1\n","        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n","    else:\n","        epochs_since_improvement = 0\n","\n","    # Save checkpoint\n","    save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n","                    decoder_optimizer, recent_bleu4, is_best)"]},{"cell_type":"code","source":["import torch.backends.cudnn as cudnn\n","import torch.optim\n","import torch.utils.data\n","import torchvision.transforms as transforms\n","from datasets import *\n","from utils import *\n","from nltk.translate.bleu_score import corpus_bleu\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import numpy as np\n"],"metadata":{"id":"KVewfBBLJkNw","executionInfo":{"status":"ok","timestamp":1653682370420,"user_tz":-180,"elapsed":7793,"user":{"displayName":"Svetlana Pavlova","userId":"05514837240559826181"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\n","# Parameters\n","data_folder = 'flickr_prep/'  # folder with data files saved by create_input_files.py\n","data_name = 'flickr30k_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n","checkpoint = 'BEST_checkpoint_flickr30k_5_cap_per_img_5_min_word_freq.pth.tar'  # model checkpoint\n","word_map_file = f'flickr_prep/WORDMAP_flickr30k_5_cap_per_img_5_min_word_freq.json'  # word map, ensure it's the same the data was encoded with and the model was trained with\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n","cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n","\n","# Load model\n","checkpoint = torch.load(checkpoint, map_location=torch.device('cpu'))\n","decoder = checkpoint['decoder']\n","#decoder = decoder.to(device)\n","decoder.eval()\n","encoder = checkpoint['encoder']\n","#encoder = encoder.to(device)\n","encoder.eval()\n","\n","# Load word map (word2ix)\n","with open(word_map_file, 'r') as j:\n","    word_map = json.load(j)\n","rev_word_map = {v: k for k, v in word_map.items()}\n","vocab_size = len(word_map)\n","\n","# Normalization transform\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225])\n","\n","\n","def evaluate(beam_size):\n","    \"\"\"\n","    Evaluation\n","\n","    :param beam_size: beam size at which to generate captions for evaluation\n","    :return: BLEU-4 score\n","    \"\"\"\n","    # DataLoader\n","    loader = torch.utils.data.DataLoader(\n","        CaptionDataset(data_folder, data_name, 'TEST', transform=transforms.Compose([normalize])),\n","        batch_size=1, shuffle=True, num_workers=1, pin_memory=True)\n","\n","    # TODO: Batched Beam Search\n","    # Therefore, do not use a batch_size greater than 1 - IMPORTANT!\n","\n","    # Lists to store references (true captions), and hypothesis (prediction) for each image\n","    # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n","    # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n","    references = list()\n","    hypotheses = list()\n","\n","    # For each image\n","    with torch.no_grad():\n","        for i, (image, caps, caplens, allcaps) in enumerate( tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n","            k = beam_size\n","            # Move to GPU device, if available\n","            image = image.to(device)  # [1, 3, 256, 256]\n","\n","            # Encode\n","            encoder_out = encoder(image)  # [1, enc_image_size=14, enc_image_size=14, encoder_dim=2048]\n","            enc_image_size = encoder_out.size(1)\n","            encoder_dim = encoder_out.size(-1)\n","            # We'll treat the problem as having a batch size of k, where k is beam_size\n","            encoder_out = encoder_out.expand(k, enc_image_size, enc_image_size, encoder_dim)  # [k, enc_image_size, enc_image_size, encoder_dim]\n","            # Tensor to store top k previous words at each step; now they're just <start>\n","            # Important: [1, 52] (eg: [[<start> <start> <start> ...]]) will not work, since it contains the position encoding\n","            k_prev_words = torch.LongTensor([[word_map['<start>']]*52] * k).to(device)  # (k, 52)\n","            # Tensor to store top k sequences; now they're just <start>\n","            seqs = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n","            # Tensor to store top k sequences' scores; now they're just 0\n","            top_k_scores = torch.zeros(k, 1).to(device)\n","            # Lists to store completed sequences and scores\n","            complete_seqs = []\n","            complete_seqs_scores = []\n","            step = 1\n","\n","            # Start decoding\n","            # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n","            while True:\n","                # print(\"steps {} k_prev_words: {}\".format(step, k_prev_words))\n","                # cap_len = torch.LongTensor([52]).repeat(k, 1).to(device) may cause different sorted results on GPU/CPU in transformer.py\n","                cap_len = torch.LongTensor([52]).repeat(k, 1)  # [s, 1]\n","                scores, _, _, _, _ = decoder(encoder_out, k_prev_words, cap_len)\n","                scores = scores[:, step-1, :].squeeze(1)  # [s, 1, vocab_size] -> [s, vocab_size]\n","                scores = F.log_softmax(scores, dim=1)\n","                # top_k_scores: [s, 1]\n","                scores = top_k_scores.expand_as(scores) + scores  # [s, vocab_size]\n","                # For the first step, all k points will have the same scores (since same k previous words, h, c)\n","                if step == 1:\n","                    top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n","                else:\n","                    # Unroll and find top scores, and their unrolled indices\n","                    top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n","\n","                # Convert unrolled indices to actual indices of scores\n","                prev_word_inds = top_k_words // vocab_size  # (s)\n","                next_word_inds = top_k_words % vocab_size  # (s)\n","\n","                # Add new words to sequences\n","                seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n","                # Which sequences are incomplete (didn't reach <end>)?\n","                incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n","                                   next_word != word_map['<end>']]\n","                complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n","                # Set aside complete sequences\n","            if len(complete_inds) > 0:\n","                complete_seqs.extend(seqs[complete_inds].tolist())\n","                complete_seqs_scores.extend(top_k_scores[complete_inds])\n","            k -= len(complete_inds)  # reduce beam length accordingly\n","\n","            # Proceed with incomplete sequences\n","            if k == 0:\n","                break\n","            seqs = seqs[incomplete_inds]\n","            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n","            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n","            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n","            k_prev_words[:, :step+1] = seqs   # [s, 52]\n","            # Break if things have been going on too long\n","            if step > 50:\n","                break\n","            step += 1\n","\n","        i = complete_seqs_scores.index(max(complete_seqs_scores))\n","        seq = complete_seqs[i]\n","\n","        # References\n","        img_caps = allcaps[0].tolist()\n","        img_captions = list(\n","            map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n","                img_caps))  # remove <start> and pads\n","        references.append(img_captions)\n","\n","        # Hypotheses\n","        hypotheses.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n","\n","        assert len(references) == len(hypotheses)\n","\n","    # Calculate BLEU-4 scores\n","    bleu4 = corpus_bleu(references, hypotheses)\n","\n","    return bleu4\n","\n","\n","if __name__ == '__main__':\n","    beam_size = 1\n","    print(\"\\nBLEU-4 score @ beam size of %d is %.4f.\" % (beam_size, evaluate(beam_size)))\n"],"metadata":{"id":"0ZUINwAYJhJl"},"execution_count":null,"outputs":[]}],"metadata":{"interpreter":{"hash":"88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"},"kernelspec":{"display_name":"Python 3.8.11 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"orig_nbformat":4,"colab":{"name":"captioneers.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}